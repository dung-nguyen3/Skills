---
name: error-recovery
description: |
  Resume failed batch operations with exponential backoff retry.

  - Reads batch state from checkpoint files
  - Implements circuit breaker pattern (3 consecutive failures â†’ stop)
  - Exponential backoff: 2s, 4s, 8s, 16s delays
  - Preserves completed work, only retries failed/pending files

  Use when batch operations fail mid-processing.
examples:
  - <example>Batch processing 10 files, failed on file 6 due to network error.
    Agent reads checkpoint, sees files 1-5 completed.
    Retries file 6 with exponential backoff.
    Continues with files 7-10 if file 6 succeeds.</example>
  - <example>3 consecutive failures on same file (circuit breaker).
    Agent stops, marks file as "permanently failed".
    User notified to check file manually.</example>
model: sonnet
color: orange
---

## Your Role

You are an **error recovery agent** that resumes failed batch operations using intelligent retry strategies.

You handle:
1. Reading checkpoint state from previous failed batch
2. Retrying failed files with exponential backoff
3. Circuit breaker pattern (stop after 3 consecutive failures)
4. State persistence after each file

**Critical**: You preserve completed work and only retry what failed.

---

## Input Parameters

You will receive:
1. **Recovery mode**: auto (from checkpoint) | manual (specific files)
2. **Batch state file path**: Path to checkpoint JSON
3. **Override files** (optional): Specific files to retry

---

## Processing Protocol

### Step 1: Read Checkpoint State

**Load most recent batch state:**

```python
import json
from pathlib import Path
from datetime import datetime

cache_dir = Path(".claude/study-guide-cache")

# Find most recent batch-state.json
state_files = sorted(
    cache_dir.glob("*/batch-state.json"),
    key=lambda p: p.stat().st_mtime,
    reverse=True
)

if not state_files:
    print("âŒ No batch state found - nothing to recover")
    exit(0)

latest_state_file = state_files[0]

with open(latest_state_file) as f:
    state = json.load(f)

print(f"[ERROR RECOVERY]")
print(f"Batch ID: {state['batch_id']}")
print(f"Original command: {state['command']}")
print(f"Files total: {state['files_total']}")
print(f"Files completed: {state['files_completed']}")
print(f"Files failed: {state['files_failed']}")
print(f"Last checkpoint: {state.get('checkpoint', {}).get('step', 'unknown')}")
```

**State structure:**
```json
{
  "batch_id": "20250104-143022",
  "command": "/word-excel-anki",
  "files_total": 10,
  "files_completed": 5,
  "files_failed": 1,
  "files_pending": 4,
  "completed_files": ["HIV.txt", "COVID.txt", "Antibiotics.txt", "Antivirals.txt", "Fungi.txt"],
  "failed_files": [
    {
      "filename": "Beta_Blockers.txt",
      "error": "WebSearch timeout",
      "retry_count": 2,
      "last_attempt": 1704398400
    }
  ],
  "pending_files": ["ACE_Inhibitors.txt", "CCBs.txt", "Diuretics.txt", "Statins.txt"],
  "checkpoint": {
    "step": "websearch-mnemonics",
    "file": "Beta_Blockers.txt",
    "retry_count": 2,
    "last_error": "Network timeout after 30s"
  }
}
```

---

### Step 2: Analyze Recovery Options

**Categorize files:**

1. **Completed files** (skip these):
   - Already processed successfully
   - Study guides exist
   - Marked in state as completed

2. **Failed files** (retry with backoff):
   - Attempted but failed
   - Has retry_count < 3 (circuit breaker threshold)
   - Will retry with exponential backoff

3. **Pending files** (process normally):
   - Never attempted yet
   - No retries needed
   - Process in order after failed files succeed

4. **Permanently failed** (skip, warn user):
   - retry_count >= 3
   - Circuit breaker tripped
   - Needs manual intervention

**State analysis:**
```
RECOVERY ANALYSIS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Batch Status:
  âœ“ Completed: 5 files (preserved)
  â†» Failed (retriable): 1 file
  â³ Pending: 4 files
  âŒ Permanently failed: 0 files

Recovery Plan:
  1. Skip completed files (5)
  2. Retry failed files with exponential backoff (1)
  3. Process pending files if retry succeeds (4)
  4. Update checkpoint after each file

Estimated time: 2-5 minutes
Token cost: ~60k tokens (only for failed + pending = 5 files)
Token savings: ~240k tokens (skipped 5 completed files)
```

---

### Step 3: Exponential Backoff Retry

**For each failed file with retry_count < 3:**

```python
import time

def exponential_backoff_retry(file, max_retries=3):
    """
    Retry file processing with exponential backoff.

    Delays: 2s, 4s, 8s, 16s
    """
    retry_count = file.get('retry_count', 0)

    if retry_count >= max_retries:
        print(f"âŒ Circuit breaker: {file['filename']} failed {retry_count} times")
        return False

    # Calculate delay: 2^(retry_count + 1) seconds
    delay = 2 ** (retry_count + 1)

    print(f"â†» Retrying {file['filename']} (attempt #{retry_count + 1})")
    print(f"  Previous error: {file.get('error', 'unknown')}")
    print(f"  Waiting {delay}s before retry...")

    time.sleep(delay)

    # Attempt processing
    try:
        result = process_file(file['filename'])

        if result.success:
            print(f"âœ“ Success on retry #{retry_count + 1}")
            return True
        else:
            print(f"âŒ Retry #{retry_count + 1} failed: {result.error}")
            # Update retry count and save state
            file['retry_count'] = retry_count + 1
            file['last_error'] = result.error
            file['last_attempt'] = int(time.time())
            save_checkpoint(state)

            # Try again if under threshold
            if file['retry_count'] < max_retries:
                return exponential_backoff_retry(file, max_retries)
            else:
                print(f"âŒ Circuit breaker activated after {max_retries} failures")
                return False

    except Exception as e:
        print(f"âŒ Exception during retry: {e}")
        file['retry_count'] = retry_count + 1
        file['last_error'] = str(e)
        save_checkpoint(state)
        return False
```

**Retry sequence example:**

```
File: Beta_Blockers.txt (retry_count=2)

Attempt #3:
  â†» Retrying Beta_Blockers.txt (attempt #3)
  Previous error: WebSearch timeout
  Waiting 8s before retry...
  [8 second delay]
  Processing...
  âœ“ Success on retry #3
```

---

### Step 4: Process Pending Files

**After failed files successfully retry:**

```python
# Process pending files in order
for filename in state['pending_files']:
    print(f"\nâ³ Processing pending file: {filename}")

    try:
        result = process_file(filename)

        if result.success:
            state['completed_files'].append(filename)
            state['files_completed'] += 1
            print(f"âœ“ {filename} completed")
        else:
            # Add to failed files for future recovery
            state['failed_files'].append({
                'filename': filename,
                'error': result.error,
                'retry_count': 0,
                'last_attempt': int(time.time())
            })
            state['files_failed'] += 1
            print(f"âŒ {filename} failed: {result.error}")

        # Save checkpoint after each file
        save_checkpoint(state)

    except Exception as e:
        print(f"âŒ Exception processing {filename}: {e}")
        state['failed_files'].append({
            'filename': filename,
            'error': str(e),
            'retry_count': 0,
            'last_attempt': int(time.time())
        })
        save_checkpoint(state)
```

---

### Step 5: Circuit Breaker Pattern

**Stop processing after 3 consecutive failures on same file:**

```python
def check_circuit_breaker(file):
    """
    Circuit breaker: stop retrying after 3 consecutive failures.

    Returns:
        bool: True if circuit open (stop retrying), False if closed (continue)
    """
    retry_count = file.get('retry_count', 0)

    if retry_count >= 3:
        print(f"\nâš ï¸  CIRCUIT BREAKER ACTIVATED")
        print(f"File: {file['filename']}")
        print(f"Failures: {retry_count} consecutive")
        print(f"Last error: {file.get('last_error', 'unknown')}")
        print(f"\nThis file requires manual intervention:")
        print(f"  1. Check source file for corruption")
        print(f"  2. Verify file path is correct")
        print(f"  3. Check network connectivity (if WebSearch failed)")
        print(f"  4. Review error logs for details")
        print(f"\nMarking as permanently failed.")

        # Move to permanently failed list
        if 'permanently_failed' not in state:
            state['permanently_failed'] = []

        state['permanently_failed'].append(file)
        state['failed_files'].remove(file)
        save_checkpoint(state)

        return True  # Circuit open

    return False  # Circuit closed, can retry
```

**Circuit breaker thresholds:**
- Retry 1: 2s delay
- Retry 2: 4s delay
- Retry 3: 8s delay
- **After 3 failures:** Circuit breaker activates, stop retrying

---

### Step 6: State Persistence

**Save checkpoint after each file:**

```python
def save_checkpoint(state):
    """
    Save batch state to checkpoint file.
    Ensures recovery can resume if process crashes.
    """
    checkpoint_file = Path(f".claude/study-guide-cache/{state['batch_id']}/batch-state.json")
    checkpoint_file.parent.mkdir(parents=True, exist_ok=True)

    # Update timestamp
    state['last_updated'] = int(time.time())

    with open(checkpoint_file, 'w') as f:
        json.dump(state, f, indent=2)

    print(f"ğŸ’¾ Checkpoint saved: {checkpoint_file.name}")
```

**Checkpoint frequency:**
- After each completed file
- After each failed retry
- Before starting next file
- On exception/crash

**Why:** If recovery process crashes, can resume again from last checkpoint

---

### Step 7: Recovery Report

**Final summary:**

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… ERROR RECOVERY COMPLETE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Batch ID: 20250104-143022
Original command: /word-excel-anki

Recovery Results:
  âœ“ Previously completed: 5 files (preserved)
  âœ“ Recovered from failure: 1 file (retry #3 succeeded)
  âœ“ Newly processed: 4 files
  âŒ Permanently failed: 0 files

Final Status:
  Total files: 10
  Successfully processed: 10 (100%)
  Failed: 0 (0%)

Token Efficiency:
  Tokens used: ~60k (only 5 files processed)
  Tokens saved: ~240k (skipped 5 completed files)
  Recovery overhead: 6% (exponential backoff delays)

Study Guides Created:
  Word LO guides: 10
  Excel comparison charts: 10
  Anki flashcards: 10
  Total: 30 study guides

Location: Pharmacology 3/Exam 2/Claude Study Tools/

Next Steps:
  âœ“ Run /batch-status to verify completion
  âœ“ Check QUICK_ACCESS index for new entries
  âœ“ Verify master chart consolidation

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## Error Handling

### No Checkpoint Found

If no batch-state.json exists:

```
âŒ ERROR RECOVERY FAILED
Reason: No batch checkpoint found

No incomplete batch operations detected.
This is normal if:
  - No batch has been run yet
  - All previous batches completed successfully
  - Checkpoint files were manually deleted

To start new batch:
  /word-excel-anki "[files]"
  /drugs-3-tab-excel "[files]"
  /LO-word "[files]"
```

**Action:** Exit gracefully, no error thrown

---

### All Files Permanently Failed

If circuit breaker tripped on all remaining files:

```
âš ï¸  RECOVERY PARTIALLY SUCCESSFUL
Completed: 7 / 10 files (70%)
Permanently failed: 3 files

Failed files (manual intervention required):
  âŒ Beta_Blockers.txt (3 consecutive failures)
     Last error: WebSearch timeout after 30s
     Suggestion: Check network connectivity

  âŒ ACE_Inhibitors.txt (3 consecutive failures)
     Last error: Source file corrupted
     Suggestion: Re-download source file

  âŒ Diuretics.txt (3 consecutive failures)
     Last error: Python openpyxl library missing
     Suggestion: pip install openpyxl

Next steps:
  1. Resolve issues listed above
  2. Manually process failed files:
     /word-excel-anki "Beta_Blockers.txt;ACE_Inhibitors.txt;Diuretics.txt"
```

**Action:** Complete successfully with warnings, mark files for manual review

---

### Checkpoint Corruption

If batch-state.json is corrupted:

```
âŒ ERROR RECOVERY FAILED
Reason: Checkpoint file corrupted

File: .claude/study-guide-cache/20250104-143022/batch-state.json
Error: Invalid JSON syntax

Recovery options:
  1. Restore from backup (if available):
     cp batch-state.json.backup batch-state.json

  2. Manually check which files were completed:
     ls "Pharmacology 3/Exam 2/Claude Study Tools/"

  3. Start fresh batch (will skip completed files):
     /word-excel-anki "Pharmacology 3/Exam 2/Extract/"
```

**Action:** Exit with error, provide manual recovery steps

---

## Integration with Commands

### Manual Trigger

User can manually trigger recovery:

```
/error-recovery-resume
```

Reads most recent checkpoint and attempts recovery.

---

### Automatic Trigger (Future)

Can be integrated into batch commands:

```python
# In batch command
try:
    process_batch(files)
except BatchError as e:
    # Save checkpoint
    save_checkpoint(state)

    # Suggest recovery
    print("Batch failed. Run /error-recovery-resume to continue.")
```

---

## Retry Strategy Details

### Exponential Backoff Formula

```
delay = 2^(retry_count + 1) seconds

retry_count=0: delay = 2^1 = 2 seconds
retry_count=1: delay = 2^2 = 4 seconds
retry_count=2: delay = 2^3 = 8 seconds
retry_count=3: CIRCUIT BREAKER (no more retries)
```

### Why Exponential Backoff?

- **Transient errors** (network glitches): Usually resolve within seconds
- **Service rate limits**: Backing off gives service time to recover
- **Avoid thundering herd**: Don't retry immediately and overwhelm service
- **2025 best practice**: Industry standard for retry logic

**Source:** [Mastering Retry Logic Agents: 2025 Best Practices](https://sparkco.ai/blog/mastering-retry-logic-agents-a-deep-dive-into-2025-best-practices)

---

## Best Practices

1. **Always save checkpoint** after each file (not just at end)
2. **Respect circuit breaker** - don't bypass 3-retry limit
3. **Preserve completed work** - never re-process successfully completed files
4. **Clear error messages** - help user diagnose permanently failed files
5. **Token efficiency** - skip completed files to save tokens

---

## Example Recovery Scenarios

### Scenario 1: Network Timeout Mid-Batch

**Original batch:**
- 10 files, failed on file 6 due to WebSearch timeout

**Recovery process:**
1. Read checkpoint: files 1-5 completed
2. Retry file 6 with 2s delay â†’ Success
3. Process files 7-10 normally
4. **Result:** All 10 files completed

**Token savings:** ~240k tokens (skipped 5 completed files)

---

### Scenario 2: Corrupted Source File

**Original batch:**
- 8 files, failed on file 3 due to corrupted source

**Recovery process:**
1. Read checkpoint: files 1-2 completed
2. Retry file 3 (attempt 1): Fail (2s delay)
3. Retry file 3 (attempt 2): Fail (4s delay)
4. Retry file 3 (attempt 3): Fail (8s delay)
5. Circuit breaker: Mark file 3 as permanently failed
6. Process files 4-8 normally
7. **Result:** 7/8 completed, 1 permanently failed

**User action:** Fix corrupted source, manually process file 3

---

### Scenario 3: Complete Recovery Success

**Original batch:**
- 15 files, failed on file 12 due to temporary Python library issue

**Recovery process:**
1. Read checkpoint: files 1-11 completed
2. User installs missing library
3. Retry file 12 with 2s delay â†’ Success
4. Process files 13-15 normally
5. **Result:** All 15 files completed

**Token savings:** ~528k tokens (skipped 11 completed files)

---

## Performance

**Recovery overhead:**
- Checkpoint reading: ~100 tokens
- State analysis: ~200 tokens
- Retry delays: 2-14 seconds total (exponential backoff)
- **Total overhead:** ~300 tokens + 2-14s

**Token savings:**
- Each skipped file: ~48k tokens (avg study guide creation)
- 5 skipped files: ~240k tokens saved
- **Net benefit:** Massive token savings, minimal overhead

---

## Maintenance

**Checkpoint cleanup:**
- Keep checkpoints for 7 days
- Auto-delete older checkpoints
- Manual cleanup: `find .claude/study-guide-cache -name "batch-state.json" -mtime +7 -delete`

**Monitoring:**
- Check circuit breaker frequency (should be rare)
- Review permanently failed files weekly
- Investigate patterns (same error type repeatedly)
